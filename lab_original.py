# -*- coding: utf-8 -*-
"""lab_original.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zPEiXOQCj8RywxhNJxoUrXOU0nw12Kb2
"""

import nibabel
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook
import seaborn as sns
import keras
from keras.models import Model
from keras.layers import *
from keras.layers.advanced_activations import LeakyReLU
from keras.models import Model
from keras.layers.normalization import BatchNormalization
from keras import backend as K
from keras.utils import plot_model
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.optimizers import Adam, RMSprop, Adagrad, Adadelta
from sklearn.metrics import mean_squared_error
from keras.models import load_model
import os
import random
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import os
import tensorflow as tf
import keras.backend.tensorflow_backend as tf_back

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
np.random.seed(0)
random.seed(0)

print("tf.__version__ is", tf.__version__)
print("tf.keras.__version__ is:", tf.keras.__version__)


def _get_available_gpus():
    """Get a list of available gpu devices (formatted as strings).

    # Returns
        A list of available GPU devices.
    """
    # global _LOCAL_DEVICES
    if tf_back._LOCAL_DEVICES is None:
        devices = tf.config.list_logical_devices()
        tf_back._LOCAL_DEVICES = [x.name for x in devices]
    return [x for x in tf_back._LOCAL_DEVICES if 'device:gpu' in x.lower()]


tf_back._get_available_gpus = _get_available_gpus

"""# Loading Brats dataset"""


class DataLoader():
    @staticmethod
    def load_3d_volume_as_array(filename):
        if '.nii' in filename:
            return DataLoader.load_nifty_volume_as_array(filename)
        elif '.mha' in filename:
            return DataLoader.load_mha_volume_as_array(filename)

    @staticmethod
    def load_mha_volume_as_array(filename):
        img = sitk.ReadImage(filename)
        nda = sitk.GetArrayFromImage(img)
        return nda

    @staticmethod
    def load_nifty_volume_as_array(filename, with_header=False):
        img = nibabel.load(filename)
        data = img.get_data()
        data = np.transpose(data, [2, 1, 0])
        if (with_header):
            return data, img.affine, img.header
        else:
            return data

    @staticmethod
    def download_data(folders, threshold=5000000, data="healthy", mode="full"):
        train_imgs = []
        count = 0

        for patient_path in folders:
            if mode == "test":
                if count > 100:
                    break

            modalities = os.listdir(patient_path)

            seg_img_name = [img for img in modalities if "seg.nii" in img][0]
            seg_img = DataLoader.load_3d_volume_as_array(os.path.join(patient_path, seg_img_name))

            num_layers = seg_img.shape[0]
            seg_lyr_sums = [int(np.sum(img)) for img in seg_img]

            modlt_imgs = []
            for train_modlt in sorted(list(set(modalities) - set([seg_img_name]))):
                print(train_modlt)
                modlt_imgs.append(DataLoader.load_3d_volume_as_array(os.path.join(patient_path, train_modlt)))
            for xth_layer in range(num_layers):

                # only grab image slices without labeled cancer
                if (data == "healthy"):
                    if seg_lyr_sums[xth_layer] == 0:
                        img_mod_list = []
                        ample_info_check = False
                        for modlt_img in modlt_imgs:
                            img_mod_list.append(modlt_img[xth_layer])
                            if (np.sum(modlt_img[xth_layer])) > 5000000:
                                ample_info_check = True
                                # tmp
                                count += 1
                                # tmp
                        if (ample_info_check):
                            train_imgs.append(img_mod_list)
                elif data == "unhealthy":
                    if seg_lyr_sums[xth_layer] > 8000:
                        img_mod_list = []
                        ample_info_check = False
                        for modlt_img in modlt_imgs:
                            img_mod_list.append(modlt_img[xth_layer])
                            if (np.sum(modlt_img[xth_layer])) > 5000000:
                                ample_info_check = True
                                count += 1
                        if (ample_info_check):
                            train_imgs.append(img_mod_list)
                else:
                    print("No such data")
        np.save("./dataset_" + data + ".npy", train_imgs)


class PreProcessor():
    """# Loading Training Data (Preprocessing)"""

    @staticmethod
    def show():
        train_imgs = np.load("./dataset.npy")
        print(train_imgs.shape)

        train_imgs = np.array(train_imgs)
        train_imgs = train_imgs.reshape(-1, 4 * 240 * 240)
        print(train_imgs.shape)
        scaler = MinMaxScaler()
        scaler.fit(train_imgs)
        train_imgs = scaler.transform(train_imgs)
        train_imgs = np.reshape(train_imgs, (-1, 4, 240, 240))
        print(train_imgs.shape)

        f1 = plt.figure(figsize=(12, 12))
        ax1 = f1.add_subplot(221)
        ax2 = f1.add_subplot(222)
        ax3 = f1.add_subplot(223)
        ax4 = f1.add_subplot(224)
        num = 10
        ax1.imshow(train_imgs[num][0], cmap="gray")
        ax2.imshow(train_imgs[num][1], cmap="gray")
        ax3.imshow(train_imgs[num][2], cmap="gray")
        ax4.imshow(train_imgs[num][3], cmap="gray")

    """# Splitting Data into training Validation and Test set"""

    @staticmethod
    def split(train_imgs, save=False, folder="./"):
        train_data, test_vald_data = train_test_split(train_imgs, test_size=0.20, random_state=100)
        vald_data, test_data = train_test_split(test_vald_data, test_size=0.30, random_state=100)

        print(train_data.shape)
        print(vald_data.shape)
        print(test_data.shape)

        # see the percentage of train validation and testdata
        print((train_data.shape[0] + vald_data.shape[0] + test_data.shape[0]))
        print(train_data.shape[0] / (train_data.shape[0] + vald_data.shape[0] + test_data.shape[0]))
        print(vald_data.shape[0] / (train_data.shape[0] + vald_data.shape[0] + test_data.shape[0]))
        print(test_data.shape[0] / (train_data.shape[0] + vald_data.shape[0] + test_data.shape[0]))
        if save:
            np.save(folder + "dataset_split.npy", np.array((train_data, vald_data, test_data)))
        return train_data, vald_data, test_data

    """#To use One image only"""

    # if training only on one image, model selecting for a network with minimum bias
    # train_imgs = [train_data[30]]
    # train_imgs = np.asarray(train_imgs)
    # train_imgs.shape

    # plt.imshow(np.reshape(train_data[5][3], (240, 240)), cmap='gray')
    #
    # plt.imshow(np.reshape(train_data[5][3], (240, 240)))


"""#Model Architecture"""


class Autoencoder():

    def __init__(self):
        inputs = Input(shape=(4, 240, 240))
        conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                       data_format='channels_first')(
            inputs)
        conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                       data_format='channels_first')(
            conv1)

        pool1 = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv1)

        conv2 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal', data_format='channels_first')(pool1)
        conv2 = BatchNormalization(axis=1)(conv2)
        conv2 = Activation(activation='relu')(conv2)

        conv2 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal', data_format='channels_first')(conv2)
        conv2 = BatchNormalization(axis=1)(conv2)
        conv2 = Activation(activation='relu')(conv2)

        pool2 = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv2)

        conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                       data_format='channels_first')(
            pool2)
        conv2 = BatchNormalization(axis=1)(conv3)
        pool3 = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv3)

        conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                       data_format='channels_first')(
            pool3)

        pool4 = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv4)

        # Latent Space, 512 * 15 * 15 = 115200

        up6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                     data_format='channels_first')(
            UpSampling2D(size=(2, 2), data_format='channels_first')(pool4))

        conv6 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal', data_format='channels_first')(
            UpSampling2D(size=(2, 2), data_format='channels_first')(up6))
        conv6 = BatchNormalization(axis=1)(conv6)
        conv6 = Activation(activation='relu')(conv6)

        up7 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                     data_format='channels_first')(
            UpSampling2D(size=(2, 2), data_format='channels_first')(conv6))
        up7 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                     data_format='channels_first')(
            up7)
        up7 = BatchNormalization(axis=1)(up7)

        up8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                     data_format='channels_first')(
            UpSampling2D(size=(2, 2), data_format='channels_first')(up7))
        up8 = BatchNormalization(axis=1)(up8)

        conv8 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                       data_format='channels_first')(
            up8)
        conv8 = BatchNormalization(axis=1)(conv8)
        conv8 = Activation(activation='relu')(conv8)

        conv8 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal',
                       data_format='channels_first')(
            conv8)
        conv8 = BatchNormalization(axis=1)(conv8)

        decoded = Conv2D(4, 3, activation='sigmoid', padding='same', data_format='channels_first')(conv8)

        self.autoencoder = Model(inputs=inputs, outputs=decoded)

    def run(self, train_data, vald_data):
        lr = [0.00001, 0.0001, 0.001]
        for i in range(0, 3):
            self.autoencoder.compile(optimizer=Adam(lr=lr[i]), loss='mean_squared_error', metrics=['accuracy'])

            checkpoint = ModelCheckpoint('Models/Best_Model_04_07_' + str(i) + '.h5', verbose=1,
                                         monitor='val_loss', save_best_only=True,
                                         mode='auto')
            early_stopping = EarlyStopping(monitor='val_loss',
                                           min_delta=0.00001,
                                           patience=8,
                                           verbose=0, mode='auto')
            # validation_data=(vald_data, vald_data),

            hist_val = self.autoencoder.fit(train_data,
                                            train_data,
                                            batch_size=3,
                                            validation_data=(vald_data, vald_data),
                                            validation_split=0,
                                            shuffle=True,
                                            epochs=200,
                                            callbacks=[checkpoint])

    def load(self, path):
        self.autoencoder.load_weights(path)


"""#Checking the Data range"""

# d = np.transpose(train_imgs.reshape(-1, 4, 240 * 240)[4])
# df = pd.DataFrame(data=d, columns=["flair", "t1", "t1ce", "t2"])
# joypy.joyplot(df, bins=40, overlap=0, grid=True)


if __name__ == '__main__':
    roots = ["E:\Lab\MICCAI_BraTS_2019_Data_Training\MICCAI_BraTS_2019_Data_Training/HGG/",
             "E:\Lab\MICCAI_BraTS_2019_Data_Training\MICCAI_BraTS_2019_Data_Training/LGG/"]

    patient_folders = [os.path.join(roots[0], p) for p in os.listdir(roots[0])] + \
                      [os.path.join(roots[1], p) for p in os.listdir(roots[1])]

    # data = DataLoader.download_data(patient_folders)
    # data = np.load("./dataset.npy")
    # train_data, vald_data, test_data = PreProcessor.split(data, save=True)

    train_data, vald_data, test_data = np.load("./dataset_split.npy", allow_pickle=True)

    model = Autoencoder()

    # model.load("./Models/Model_30_06.h5")

    model.run(train_data[0:100], vald_data[0:100])

    autoencoder = load_model("./Models/Best_Model.h5")

# Everything below, I dont need for now.

if False:
    """#Orignal Image"""

    plt.imshow(np.reshape(vald_data[30][1], (240, 240)), cmap='gray')

    plt.imshow(np.reshape(vald_data[30][0], (240, 240)))

    plt.imshow(np.reshape(train_imgs[40][2], (240, 240)), cmap="gray")

    """# Model output"""

    plt.imshow(np.reshape(output[:, 2, :, :], (240, 240)))

    """# Model Performance with Mean Squared Error"""

    # plt.plot(hist_val.history["loss"])
    # plt.plot(hist_val.history["val_loss"])
    # plt.xlabel("Epochs")
    # plt.ylabel("Loss")
    # plt.legend()
    # plt.savefig("Colab Notebooks/Model_Performance.png")
    # plt.plot()

    """# Loading save model"""

    autoencoder = load_model("./Colab Notebooks/Models/Best_Model.h5")

    """#Test Data (Healthy Images) Error Analysis"""

    fig = plt.figure(figsize=(12, 12))
    ax = fig.add_subplot(111)
    ax.plot(np.array(mse))
    ax.set_xlabel("No of Images")
    ax.set_ylabel("MSE Loss")
    ax.yaxis.set_ticks(np.arange(0.0, 0.0040, 0.0005))
    ax.legend(["Model Performance Test Data"])
    plt.savefig("Colab Notebooks/Test_data_MSE_Loss.png")
    plt.show()

    """#Loading Unhealthy Images"""

    roots = ["Colab Notebooks/med/HGG/", "Colab Notebooks/med/LGG/"]

    threshold = 10.0
    unhealthy_test_imgs = []
    count = 0

    patient_folders = [os.path.join(roots[0], p) for p in os.listdir(roots[0])] + \
                      [os.path.join(roots[1], p) for p in os.listdir(roots[1])]

    unhealthy_test_imgs = DataLoader.download_data(patient_folders, data="unhealthy")

    unhealthy_test_imgs = np.asarray(unhealthy_test_imgs)
    unhealthy_test_imgs.shape

    """# Saving Unhealthy Dataset"""

    unhealthy_test_imgs = np.asarray(unhealthy_test_imgs)
    np.save("./Colab Notebooks/test_unhealthy_images_dataset.npy", unhealthy_test_imgs)
    unhealthy_test_imgs.shape

    unhealthy_test_imgs = np.load("./Colab Notebooks/test_unhealthy_images_dataset.npy")

    """# Applying Preprocessing steps"""

    unhealthy_test_imgs = unhealthy_test_imgs.reshape(-1, 4 * 240 * 240)
    print(unhealthy_test_imgs.shape)
    scaler = MinMaxScaler()
    scaler.fit(unhealthy_test_imgs)
    unhealthy_test_imgs = scaler.transform(unhealthy_test_imgs)
    unhealthy_test_imgs = np.reshape(unhealthy_test_imgs, (-1, 4, 240, 240))
    print(unhealthy_test_imgs.shape)

    """# Method to calculate Mean Squared Error"""


    def calculate_mse(data):
        predicted_mse = []
        print("Total images", len(data))
        for index, j in enumerate(data):
            predicted = autoencoder.predict(np.expand_dims(data[index], 0))
            predicted = predicted.reshape(4, 240 * 240)
            Actual = data[index].reshape(4, 240 * 240)
            mse = mean_squared_error(Actual, predicted)
            predicted_mse.append(mse)

        return predicted_mse


    # healthy validation data
    healthy_mse_vald = calculate_mse(vald_data)
    healthy_true_label_vald = np.ones(len(vald_data))
    healthy_mse_vald = np.array(healthy_mse_vald)

    # healthy test data
    healthy_mse = calculate_mse(test_data)
    healthy_true_label = np.ones(len(test_data))
    healthy_mse = np.array(healthy_mse)

    # unhealthy data
    unhealthy_mse = calculate_mse(unhealthy_test_imgs)
    unhealthy_true_label = np.zeros(len(unhealthy_test_imgs))
    unhealthy_mse = np.array(unhealthy_mse)

    # load for calculating statistics
    np.save("healthy_mse_vald.npy", healthy_mse_vald)
    np.save("healthy_mse.npy", healthy_mse)
    np.save("unhealthy_mse.npy", unhealthy_mse)

    healthy_mse_vald = np.load("Colab Notebooks/healthy_mse_vald.npy")
    healthy_true_label_vald = np.ones(len(healthy_mse_vald))
    healthy_mse = np.load("Colab Notebooks/healthy_mse.npy")
    healthy_true_label = np.ones(len(healthy_mse))
    unhealthy_mse = np.load("Colab Notebooks/unhealthy_mse.npy")
    unhealthy_true_label = np.zeros(len(unhealthy_mse))

    # box plot showing distributions of Healthy brain images (Test set) and Unhealthy brain images
    # note: we do not calculate anything here, we just visualise the difference in distributions

    fig, ax = plt.subplots(figsize=(8, 8))
    ax.boxplot([healthy_mse, unhealthy_mse], labels=["Healthy brain images (Test set)", "Unhealthy brain images"])
    plt.ylabel("Reconstruction loss")
    plt.savefig("Colab Notebooks/box_plot.png")

    """# Calculating Healthy test data, Unhealthy data reconstruction loss and code for finding optimal threshold"""

    # Dividing the unhealthy images into two parts, "unhealthy_true_label1" and "unhealthy_true_label2"
    # use one part for finding the optimal thrushold and use second part to calculate the F1 value and confusion matrix

    unhealthy_mse1, unhealthy_mse2 = train_test_split(unhealthy_mse, test_size=0.5, random_state=100)
    unhealthy_true_label1, unhealthy_true_label2 = train_test_split(unhealthy_true_label, test_size=0.5,
                                                                    random_state=100)


    # get try diffrent value of thrushold and f1 values
    def get_roc_dict(hlty_mse, hlty_tl, unhlty_mse, unhlty_tl,
                     upper_limit=0.0006, lower_limit=0.0, step_size=0.000025):
        roc_dict = {}
        thrs = lower_limit

        while thrs <= upper_limit:
            total_pred = list((hlty_mse < thrs).astype(int)) + list((unhlty_mse < thrs).astype(int))
            total_true = list(hlty_tl) + list(unhlty_tl)
            f1 = f1_score(total_pred, total_true)
            roc_dict[f1] = {
                "total_pred": total_pred,
                "total_true": total_true,
                "thrs": thrs
            }
            thrs = thrs + step_size

        return roc_dict


    """# Generating Confusion Matrix"""

    # we use the healthy images from the validation set and one part of unhealthy images data "unhealthy_true_label1" to find the optimal thrushold
    roc_dict_valid = get_roc_dict(healthy_mse_vald, healthy_true_label_vald, unhealthy_mse1, unhealthy_true_label1)
    best_f1 = sorted(list(roc_dict_valid.keys()))[-1]
    thrs = roc_dict_valid[best_f1]["thrs"]
    print("thrus", thrs)

    # using the optimal thrushold we classify the healthy images from test data (so far unseen) and second half of unhealthy
    # images data "unhealthy_true_label2" (also not used untill now)

    total_pred = list((healthy_mse < thrs).astype(int)) + list((unhealthy_mse2 < thrs).astype(int))
    total_true = list(healthy_true_label) + list(unhealthy_true_label2)

    # make the confusion matrix and f1 score on this pridictions

    matrix = confusion_matrix(total_pred, total_true)
    print(matrix)

    f1 = f1_score(total_pred, total_true)
    print('F1 score: %f' % f1)

    """# Methods for generating ROC curve"""

    # Make a ROC curve on the healthy test data and second half of unhealthy data

    roc_dict = get_roc_dict(healthy_mse, healthy_true_label, unhealthy_mse2, unhealthy_true_label2)

    true_posi_rate_list = []
    false_posi_rate_list = []
    for key in roc_dict.keys():
        pred_posi, pred_neg = confusion_matrix(roc_dict[key]["total_pred"], roc_dict[key]["total_true"])
        # [true_posi, false_posi]
        # [false_neg, true_neg]

        true_posi, false_posi = pred_posi
        false_neg, true_neg = pred_neg

        true_posi_rate = true_posi / (true_posi + false_neg)
        false_posi_rate = false_posi / (false_posi + true_neg)
        true_posi_rate_list.append(true_posi_rate)
        false_posi_rate_list.append(false_posi_rate)

    # roc curve
    fig, ax = plt.subplots(figsize=(8, 8))
    plt.plot(false_posi_rate_list, true_posi_rate_list)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.savefig("roc.png")
    plt.savefig("Colab Notebooks/roc.png")

    # old, just to see diffence bw heatlhy test data (orange) health validation data (blue) and unhealth data (green)

    fig = plt.figure(figsize=(12, 12))
    ax = fig.add_subplot(111)
    ax.plot(np.array(mse))
    ax.set_xlabel("No of Images")
    ax.set_ylabel("MSE Loss")
    # ax.yaxis.set_ticks(np.arange(0.0, 0.0040, 0.0005))
    ax.legend(["Model Performance Unhealthy images"])
    # plt.savefig("Colab Notebooks/Test_data_MSE_Loss.png")
    plt.show()
